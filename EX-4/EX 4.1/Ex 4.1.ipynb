{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'c'), ('c', 'd'), ('d', 'c'), ('c', 'e'), ('e', 'f')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]\n",
    "from nltk.util import bigrams\n",
    "list(bigrams(text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'a', 'c', 'd', 'c', 'e', 'f', '</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
    "#from nltk.util import pad_both_ends\n",
    "list(pad_sequence(text[1],pad_left=True,left_pad_symbol=\"<s>\",pad_right=True,right_pad_symbol=\"</s>\",n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'a'),\n",
       " ('a', 'c'),\n",
       " ('c', 'd'),\n",
       " ('d', 'c'),\n",
       " ('c', 'e'),\n",
       " ('e', 'f'),\n",
       " ('f', '</s>')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(bigrams(pad_sequence(text[1],pad_left=True,left_pad_symbol=\"<s>\",pad_right=True,right_pad_symbol=\"</s>\",n=2)))\n",
    "list(bigrams(pad_both_ends(text[1], n=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'a',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'c',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'd',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'c',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'e',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'f',\n",
       " '</s>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flatten(pad_both_ends(sent, n=2) for sent in text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'c', 'd', 'c', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(2, text[1])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "lm = MLE(2)\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 8 items>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'c', 'd', 'c', 'e', 'f')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', '<UNK>', '<UNK>')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup([\"a\", \"from\", \"Mars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 30 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts[['a']]['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05555555555555555\n",
      "0.0\n",
      "-4.169925001442313\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "print(lm.score(\"a\"))\n",
    "print(lm.score(\"b\", [\"a\"]))\n",
    "print(lm.logscore(\"a\"))\n",
    "print(lm.logscore(\"b\", [\"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(\"<UNK>\") == lm.score(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "test = [('a', 'b'), ('c', 'd')]\n",
    "print(lm.entropy(test))\n",
    "print(lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "['</s>', '<s>', 'c', '</s>', '<s>', 'a', '</s>', 'd', '</s>', '</s>', 'f', '</s>', 'd', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm.generate(1, random_seed=3))\n",
    "print(lm.generate(15, random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '<s>', 'c', '</s>', '<s>', 'a', '</s>', 'd', '</s>', '</s>', 'f', '</s>', 'd', '</s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "print(lm.generate(15, text_seed=['c'], random_seed=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Universal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "The File contents\n",
      "We are surrounded by language in all aspects of our lives. Language is the basis of our very existence, something that makes us who we are. Language has enabled us to do things in such a simplistic manner that would otherwise had been impossible, like communicating ideas, telling gigantic stories like the Lord of the Rings and even gain a deeper insight into our inner selves.\n",
      "Tokens\n",
      "['We', 'are', 'surrounded', 'by', 'language', 'in', 'all', 'aspects', 'of', 'our', 'lives', '.', 'Language', 'is', 'the', 'basis', 'of', 'our', 'very', 'existence', ',', 'something', 'that', 'makes', 'us', 'who', 'we', 'are', '.', 'Language', 'has', 'enabled', 'us', 'to', 'do', 'things', 'in', 'such', 'a', 'simplistic', 'manner', 'that', 'would', 'otherwise', 'had', 'been', 'impossible', ',', 'like', 'communicating', 'ideas', ',', 'telling', 'gigantic', 'stories', 'like', 'the', 'Lord', 'of', 'the', 'Rings', 'and', 'even', 'gain', 'a', 'deeper', 'insight', 'into', 'our', 'inner', 'selves', '.']\n",
      "------------------------------------------------\n",
      "55\n",
      "[('of', 3), ('our', 3), ('.', 3), ('the', 3), (',', 3), ('are', 2), ('in', 2), ('Language', 2), ('that', 2), ('us', 2)]\n"
     ]
    }
   ],
   "source": [
    "#Excerise 1\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#file = open(\"input_text.txt\",\"rt\")\n",
    "file = open(\"input_1.txt\",\"rt\")\n",
    "text=file.read()\n",
    "file.close()\n",
    "print(\"The File contents\")\n",
    "print(text)\n",
    "\n",
    "tokens=word_tokenize(text)\n",
    "print(\"Tokens\")\n",
    "print(tokens)\n",
    "print(\"------------------------------------------------\")\n",
    "#--------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------\n",
    "#sentences = sent_tokenize(text) \n",
    "#print(\"Sentences \",sentences)\n",
    "#text=sentences\n",
    "#print(\"------------------------------------------------\")\n",
    "#-------------------------------------------------\n",
    "\n",
    "wfreq = nltk.FreqDist(tokens)\n",
    "print(len(wfreq))  #Prints total number of unique words in the text document\n",
    "print(wfreq.most_common(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "sentcount = wfreq['.'] + wfreq['?'] + wfreq['!']  # Assuming every sentence ends with ., ! or \n",
    "print(sentcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.181818181818182\n"
     ]
    }
   ],
   "source": [
    "#Average length of a sentence \n",
    "sentence = \"NLP program to learn the basics of statistical applications on text\"\n",
    "words = sentence.split()\n",
    "average = sum(len(word) for word in words) / len(words)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "text=text.lower()\n",
    "print(\"After converting to Lower cases\")\n",
    "print(text)\n",
    "print(\"------------------------------------------------\")\n",
    "#---------------------------------------------------\n",
    "num_remo = re.sub(r'\\d+', '', text)\n",
    "print(\"After removal of Numbers\")\n",
    "print(num_remo)\n",
    "print(\"------------------------------------------------\")\n",
    "#-------------------------------------------------\n",
    "translator = str.maketrans('', '', string.punctuation) \n",
    "text = num_remo.replace('-', ' ')\n",
    "pun_rem=text.translate(translator)\n",
    "print(\"After Punctuation removal\")\n",
    "print(pun_rem)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "tokens=word_tokenize(pun_rem)\n",
    "print(\"Tokens\")\n",
    "print(tokens)\n",
    "print(\"------------------------------------------------\")\n",
    "#--------------------------------------------------\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "#print(\"After Lemmatizing on Noun -->\",lemmatized_output)\n",
    "lemmatized_output=word_tokenize(lemmatized_output)\n",
    "lemmatized_output_1 = [lemmatizer.lemmatize(w,'v') for w in lemmatized_output]\n",
    "print(\"After Lemmatizing \")\n",
    "print(lemmatized_output_1)\n",
    "print(\"------------------------------------------------\")\n",
    "#------------------------------------------\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence = [w for w in lemmatized_output_1 if not w in stop_words]\n",
    "print(\"After Removing the stop words\")\n",
    "print(filtered_sentence)\n",
    "print(\"------------------------------------------------\")\n",
    "text = pos_tag(filtered_sentence)\n",
    "print(\"After PoS attachment:\",text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
